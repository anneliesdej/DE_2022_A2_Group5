{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfdcbb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ParentLocation: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Period: long (nullable = true)\n",
      " |-- isLatestYear: string (nullable = true)\n",
      " |-- Dim1: string (nullable = true)\n",
      " |-- FactValueNumeric: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType, DateType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "#set up spark (session)\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"BatchPipeline\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "#set up hadoop fs configuration\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "#retrieve data from bucket\n",
    "#google storage file path\n",
    "#make sure this is a separate bucket that only has the correct files in there\n",
    "gsc_file_path = 'gs://jadsdenb/' # bucket name with whodata.csv file\n",
    "\n",
    "dataSchema = StructType(\n",
    "    [StructField(\"ParentLocation\", StringType(), True),\n",
    "    StructField(\"Location\", StringType(), True),\n",
    "    StructField(\"Period\", LongType(), True),\n",
    "    StructField(\"isLatestYear\", StringType(), True),\n",
    "    StructField(\"Dim1\", StringType(), True),\n",
    "    StructField(\"FactValueNumeric\", DoubleType(), True)\n",
    "    ])\n",
    "\n",
    "consumed_import = spark.read.format(\"csv\").schema(dataSchema).option(\"header\", \"true\") \\\n",
    "    .load(gsc_file_path+'whodata.csv')\n",
    "\n",
    "consumed_import.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "739a21a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+--------------+\n",
      "|             Country|Year|Litres_alcohol|\n",
      "+--------------------+----+--------------+\n",
      "|United States of ...|1960|          7.83|\n",
      "|             Austria|1960|          8.91|\n",
      "|         Afghanistan|1961|           0.0|\n",
      "|              Guinea|1961|          0.24|\n",
      "|Micronesia (Feder...|1961|           0.0|\n",
      "|                Oman|1961|           0.0|\n",
      "|          Bangladesh|1961|           0.0|\n",
      "|              Kuwait|1961|          0.02|\n",
      "|               Nepal|1961|           0.0|\n",
      "|          Mauritania|1961|          0.11|\n",
      "|               Qatar|1961|           0.0|\n",
      "|        Saudi Arabia|1961|           0.0|\n",
      "|            Pakistan|1961|          0.01|\n",
      "|           Indonesia|1961|          0.03|\n",
      "|              Malawi|1961|          0.05|\n",
      "|             Somalia|1961|          0.06|\n",
      "|             Comoros|1961|          0.12|\n",
      "|               Niger|1961|          0.13|\n",
      "|Iran (Islamic Rep...|1961|          0.14|\n",
      "|              Jordan|1961|          0.14|\n",
      "+--------------------+----+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "consumed = consumed_import.select(col('Location').alias('Country') , col('Period').alias('Year'), col('FactValueNumeric').alias('Litres_alcohol')) \\\n",
    "    .where(col('Dim1') == 'All types')\\\n",
    "    .sort('Year') \\\n",
    "\n",
    "consumed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95ebe83b-b6ad-4190-8d5e-e61e1f59c96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------------+\n",
      "|Year|Total yearly consumption|\n",
      "+----+------------------------+\n",
      "|1960|      16.740000000000002|\n",
      "|1961|                   531.0|\n",
      "|1962|       557.3200000000002|\n",
      "|1963|       611.7099999999997|\n",
      "|1964|                  624.44|\n",
      "|1965|       633.9300000000001|\n",
      "|1966|       660.0799999999997|\n",
      "|1967|       664.3100000000004|\n",
      "|1968|       675.6399999999999|\n",
      "|1969|       687.0300000000001|\n",
      "|1970|       719.9400000000002|\n",
      "|1971|                  732.55|\n",
      "|1972|       746.9100000000001|\n",
      "|1973|       783.8900000000002|\n",
      "|1974|                  786.51|\n",
      "|1975|       796.0600000000003|\n",
      "|1976|       795.9800000000002|\n",
      "|1977|       802.9100000000003|\n",
      "|1978|       792.9999999999995|\n",
      "|1979|       796.5499999999996|\n",
      "+----+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yearly_ww_consumed = consumed.select('Year', 'Litres_alcohol') \\\n",
    "    .groupBy('Year') \\\n",
    "    .agg(sum('Litres_alcohol').alias('Total yearly consumption'))\n",
    "\n",
    "yearly_ww_consumed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3c6670f-79b9-40f3-9e61-9a79266356e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write data to bucket\n",
    "consumed.write.mode(\"overwrite\").format(\"csv\").save(\"gs://jadsdenb/consumed\") # bucket name !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9869848a-dc5a-4341-b793-0781d6caf522",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the cloud storage bucket for temporary BigQuery export data used by the connector\n",
    "bucket = \"niels_bq_bucket\" # bucket name !\n",
    "spark.conf.set('temporaryGcsBucket', bucket)\n",
    "\n",
    "#save the combined matches data to BigQuery -> do not forget to change project ID\n",
    "yearly_ww_consumed.write.format('bigquery') \\\n",
    "    .option('table', 'de2022-366418.assignment2dataset.yearly_consumption') \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "#save the goal data to bigQuery -> do not forget to change project ID\n",
    "consumed.write.format('bigquery') \\\n",
    "    .option('table', 'de2022-366418.assignment2dataset.consumption') \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fb91eed-6056-4065-aa0b-7562be42d014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the spark context\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
